{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rhw/.pyenv/versions/3.10.12/envs/musicgen/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from ast import List\n",
    "import bz2\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from transformers import AutoModelForTextEncoding, AutoModel, MusicgenForCausalLM\n",
    "import warnings\n",
    "import omegaconf\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import typing as tp\n",
    "from transformers import AutoProcessor, MusicgenForConditionalGeneration, MusicgenProcessor\n",
    "from transformers.optimization import AdamW\n",
    "\n",
    "from transformers.models.musicgen.modeling_musicgen import shift_tokens_right\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "from audiocraft.data.info_audio_dataset import AudioInfo\n",
    "from audiocraft.data.music_dataset import MusicDataset, MusicInfo, Paraphraser, augment_music_info_description\n",
    "from torch.utils.data import DataLoader\n",
    "from audiocraft.models.builders import get_conditioner_provider\n",
    "\n",
    "from audiocraft.modules.conditioners import AttributeDropout, ClassifierFreeGuidanceDropout, ConditioningAttributes, ConditioningProvider, SegmentWithAttributes, WavCondition\n",
    "from audiocraft.solvers.builders import DatasetType, get_audio_datasets\n",
    "from audiocraft.solvers.compression import CompressionSolver\n",
    "# from audiocraft.utils.cache import CachedBatchLoader, CachedBatchWriter\n",
    "from audiocraft.utils.utils import get_dataset_from_loader\n",
    "from accelerate import Accelerator, cpu_offload\n",
    "\n",
    "\n",
    "cfg = omegaconf.OmegaConf.merge(\n",
    "    omegaconf.OmegaConf.load(\"config/solver/musicgen/default.yaml\"),\n",
    "    omegaconf.OmegaConf.load(\"config/solver/default.yaml\"),\n",
    "    omegaconf.OmegaConf.load(\"config/model/lm/default.yaml\"),\n",
    "    omegaconf.OmegaConf.load(\"config/solver/compression/default.yaml\"),\n",
    "    omegaconf.OmegaConf.load(\"config/solver/compression/encodec_musicgen_32khz.yaml\"),\n",
    "    omegaconf.OmegaConf.load(\"config/solver/musicgen/musicgen_melody_32khz.yaml\"),\n",
    "    omegaconf.OmegaConf.load(\"config/model/lm/musicgen_lm.yaml\"),\n",
    "    omegaconf.OmegaConf.load(\"config/config.yaml\"),\n",
    "    omegaconf.OmegaConf.load(\"config/conditioner/chroma2music.yaml\"),\n",
    "    omegaconf.OmegaConf.load(\"config/dset/audio/example.yaml\")   \n",
    ")\n",
    "cfg.dataset.segment_duration = 30\n",
    "\n",
    "cache_path = \"./cache_dir\"\n",
    "cache_path = None\n",
    "use_cached_writer = False\n",
    "use_cached_reader = False\n",
    "\n",
    "cached_batch_writer = None\n",
    "cached_batch_loader = None\n",
    "# if cache_path is not None and use_cached_writer:\n",
    "#     cached_batch_writer = CachedBatchWriter(Path(cache_path))\n",
    "# else:\n",
    "#     cached_batch_loader = CachedBatchLoader(\n",
    "#         Path(cache_path), cfg.dataset.batch_size, cfg.dataset.num_workers,\n",
    "#         min_length=cfg.optim.updates_per_epoch or 1)\n",
    "\n",
    "cfg_dropout_p = 0.1 \n",
    "attribute_dropout_p = {\"default\": 0.1}\n",
    "# cfg_dropout = ClassifierFreeGuidanceDropout(p=cfg_dropout_p)\n",
    "# att_dropout = AttributeDropout(p=attribute_dropout_p)\n",
    "current_stage = \"train\"\n",
    "device = \"cpu\"\n",
    "cfg.device = device\n",
    "\n",
    "# condition_provider = get_conditioner_provider(cfg[\"transformer_lm\"][\"dim\"], cfg)\n",
    "# compression_model = CompressionSolver.wrapped_model_from_checkpoint(cfg, cfg.compression_model_checkpoint, device=device)\n",
    "\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from collections import defaultdict\n",
    "\n",
    "class PreparedDataset(Dataset):\n",
    "    def  __init__(self, path) -> None:\n",
    "        super().__init__()\n",
    "        self.path = Path(path) \n",
    "        self.files = list(self.path.iterdir())\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.files)\n",
    "    \n",
    "    def read_file(self, filename: tp.Union[str, Path]) -> dict[str, torch.Tensor]:\n",
    "        try:\n",
    "            # with bz2.BZ2File(filename, \"rb\") as compressed_data_file:\n",
    "            #     data = pickle.load(compressed_data_file)\n",
    "            # return data\n",
    "            return np.load(filename)\n",
    "        except:\n",
    "            print(filename)\n",
    "        \n",
    "    def __getitem__(self, index) -> dict[str, torch.Tensor]:\n",
    "        values = dict(self.read_file(self.files[index]))\n",
    "        # print(values.keys())\n",
    "        return values\n",
    "\n",
    "    @staticmethod\n",
    "    def pad_sequences(samples: List[np.ndarray], pad_value: int) -> List[np.ndarray]:\n",
    "        max_len = max(sample.shape[0] for sample in samples)\n",
    "        for idx, sample in enumerate(samples):\n",
    "            res = np.zeros((max_len, *sample.shape[1:]), dtype=sample.dtype) + pad_value\n",
    "            res[:sample.shape[0]] = sample \n",
    "            samples[idx] = res \n",
    "        return samples \n",
    "    \n",
    "    @staticmethod\n",
    "    def collate_fn(samples: List[dict[str, torch.Tensor]], use_cls: bool = False) -> dict[str, torch.Tensor]:\n",
    "        \n",
    "        for idx, att_mask in enumerate(PreparedDataset.pad_sequences([s[\"attention_mask\"] for s in samples], 0)):\n",
    "            samples[idx][\"attention_mask\"] = att_mask\n",
    "\n",
    "        for idx, ehs in enumerate(PreparedDataset.pad_sequences([s[\"encoder_hidden_states\"] for s in samples], 0)):\n",
    "            samples[idx][\"encoder_hidden_states\"] = ehs\n",
    "\n",
    "        if use_cls:\n",
    "            for idx, att_mask in enumerate(PreparedDataset.pad_sequences([s[\"attention_mask_cls\"] for s in samples], 0)):\n",
    "                samples[idx][\"attention_mask_cls\"] = att_mask\n",
    "                \n",
    "            for idx, ehs in enumerate(PreparedDataset.pad_sequences([s[\"encoder_hidden_states_cls\"] for s in samples], 0)):\n",
    "                samples[idx][\"encoder_hidden_states_cls\"] = ehs\n",
    "\n",
    "        res = defaultdict(list)\n",
    "        for sample in samples:\n",
    "            for k, v in sample.items():\n",
    "                res[k].append(v)\n",
    "        return {k: torch.from_numpy(np.array(v)) for k, v in res.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-small\").decoder\n",
    "config = decoder.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1 \n",
    "use_cls_loss = True\n",
    "dataset = PreparedDataset(\"generated_train_dataset\")\n",
    "dataloader = DataLoader(dataset, shuffle=True, batch_size=batch_size, num_workers=8, collate_fn=partial(dataset.collate_fn, use_cls=use_cls_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[2048, 1817, 1883,  ..., 1087,  277,  327],\n",
       "          [2048, 1200, 1737,  ..., 1540,    9, 1556],\n",
       "          [2048, 1525, 1056,  ..., 1390, 1453, 2006],\n",
       "          [2048, 1286,  944,  ...,  225, 1036,  606]]], dtype=torch.int32),\n",
       " tensor([[[1817, 1883, 1366,  ...,  277,  327,  236],\n",
       "          [1200, 1737,   73,  ...,    9, 1556, 1148],\n",
       "          [1525, 1056, 1146,  ..., 1453, 2006, 1674],\n",
       "          [1286,  944, 1706,  ..., 1036,  606,  939]]], dtype=torch.int32))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"input_ids\"], data[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 1500]), torch.Size([1, 4, 1500, 2048]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"labels\"].shape, data[\"labels_cls\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1500]), torch.Size([1500, 2048]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"labels\"][:, 0, ...].view(-1).shape, data[\"labels_cls\"][:, 0, ...].view(-1, data[\"labels_cls\"].size(-1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.8272)\n"
     ]
    }
   ],
   "source": [
    "ce = 0\n",
    "for i in range(4):\n",
    "    ce += F.cross_entropy(data[\"labels_cls\"][:, i, ...].view(-1, data[\"labels_cls\"].size(-1)), data[\"labels\"][:, i, ...].view(-1).long())\n",
    "ce /= 4 \n",
    "print(ce)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.8272)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(data[\"labels_cls\"].view(-1, config.vocab_size), data[\"labels\"].view(-1).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 166, 1799,  457,  ..., 1773,  987, 1280])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"labels_cls\"].view(-1, config.vocab_size).softmax(1).argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 1, 1, 1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"padding_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'attention_mask_cls', 'encoder_hidden_states', 'encoder_hidden_states_cls', 'labels', 'labels_cls', 'padding_mask'])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
